{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr/>\n",
    "\n",
    "# Data Mining\n",
    "**Tamás Budavári** - budavari@jhu.edu <br/>\n",
    "\n",
    "- Classification problems\n",
    "- Naive Bayes Classifier\n",
    "- Quadratic Discriminant Analysis\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Classification</font></h1>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Based on a **training set** of labeled points, assign class labels to unknown vectors in the **query set**.  \n",
    "\n",
    "> **Training set**\n",
    "\n",
    ">$T = \\big\\{ (\\boldsymbol{x}_i, C_i) \\big\\}_{i=1}^N$ where $x_i\\in \\mathbb{R}^d$ and $C_i$ is the known class membership \n",
    "\n",
    "> **Query set**\n",
    "\n",
    ">$Q = \\big\\{ \\boldsymbol{x}_j \\big\\}_{j=1}^M$ where $x_j\\in \\mathbb{R}^d$ \n",
    "\n",
    "> For example,\n",
    "> blood test results ($\\boldsymbol{x}$) and sick/healty ($C$) - we want to predict if a new patient is sick based on the available measurements\n",
    "\n",
    "- Similar to regression but with discrete categories to classify into..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Methods\n",
    "\n",
    "- [$k$-NN](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/lda_qda.html)\n",
    "- [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [Decisions trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Random forests](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees)\n",
    "- [Support Vector Machines](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Iris Dataset\n",
    "\n",
    "We'll use this data set available in [scikit-learn](http://scikit-learn.org/stable/index.html), see [this](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) page for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples\n",
    "\n",
    "More [exercises](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#examples-using-sklearn-neighbors-kneighborsclassifier) are available at http://scikit-learn.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$ Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique known classes in training set\n",
    "classes = np.unique(iris.target)\n",
    "print ('There are %d classes:' % len(classes), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFBBBB', '#BBFFBB', '#BBBBFF'])\n",
    "cmap_bold = ListedColormap(['#CC0000', '#00AA00', '#0000CC'])\n",
    "\n",
    "# Number of neighbors\n",
    "n_neighbors = 15\n",
    "\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,:2]  # we only take the first two features\n",
    "y = iris.target\n",
    "\n",
    "# step size in the mesh\n",
    "h = 0.05\n",
    "\n",
    "x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    \n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    ZZ = clf.predict(grid)\n",
    "    ZZ = ZZ.reshape(xx.shape) # 2-D grid layout\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each grid point\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Meaningful Distance?\n",
    "\n",
    "- Need a [distance function](https://en.wikipedia.org/wiki/Distance#Mathematics)\n",
    "\n",
    "> E.g., use Euclidean distance in $\\mathbb{R}^d$\n",
    "\n",
    "- Some examples\n",
    "\n",
    "> $ \\displaystyle \\lVert X - Y \\rVert_1 = \\sum_{i=1}^{n} \\lvert x_i - y_i \\rvert $\n",
    ">\n",
    "> $ \\displaystyle \\lVert X - Y \\rVert_2 = \\left( \\sum_{i=1}^{n} \\lvert x_i - y_i \\rvert^2 \\right)^{1/2} $\n",
    ">\n",
    "> $ \\displaystyle \\lVert X - Y \\rVert_p = \\left( \\sum_{i=1}^{n} \\lvert x_i - y_i \\rvert^p \\right)^{1/p} \\qquad 1 \\le p < \\infty $\n",
    ">\n",
    "> $ \\displaystyle \\lVert X - Y \\rVert_\\infty = \\max_{i=1, \\cdots, n} \\lvert x_i - y_i \\rvert $\n",
    "\n",
    "- Problem with different features and units\n",
    "\n",
    "> In practice, **centering** and **scaling** often helps <br/>\n",
    "> Arguably, black art...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "- In general, we can use [Bayes' rule](https://en.wikipedia.org/wiki/Bayes%27_theorem#Statement_of_theorem) (and law of total probability) to infer discrete classes $C_k$ for a given $\\boldsymbol{x}$ set of features\n",
    "\n",
    ">$\\displaystyle P(C_k \\lvert\\,\\boldsymbol{x}) = \\frac{\\pi(C_k)\\,{\\cal{}L}_{\\boldsymbol{x}}(C_k)}{Z} $ \n",
    "\n",
    "\n",
    "- Naively assuming the features are independent \n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) = \\prod_{\\alpha}^d p(x_{\\alpha} \\lvert C_k)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Learning\n",
    "\n",
    "- Say for Gaussian likelihoods, we simply estimate the sample mean and variance of all features for each class $k$\n",
    "\n",
    ">$\\displaystyle p(x_{\\alpha} \\lvert C_k) = G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})$\n",
    "\n",
    "- We have to also pick some prior for the classes\n",
    "\n",
    "> Using uniform or based on frequency of points in the training set?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes: Estimation\n",
    "\n",
    "- Look for maximum of the posterior\n",
    "\n",
    "\n",
    ">$\\displaystyle \\hat{k} =  \\mathrm{arg}\\max_k \\left[ \\pi_k \\prod_{\\alpha}^d G(x_{\\alpha};\\mu_{k,\\alpha}, \\sigma^2_{k,\\alpha})\\right]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature means and variances for each class\n",
    "param = dict()  # we save them in this dictionary\n",
    "for k in classes:\n",
    "    members = (iris.target == k) # boolean array\n",
    "    num = members.sum()    # True:1, False:0\n",
    "    prior = num / float(iris.target.size)\n",
    "    X = iris.data[members,:] # slice out members\n",
    "    mu = X.mean(axis=0)      # calc mean\n",
    "    X -= mu\n",
    "    var = (X*X).sum(axis=0) / (X[:,0].size-1)\n",
    "    param[k] = (num, prior, mu, var) # save results\n",
    "    print (k, mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init predicted values\n",
    "k_pred = -1 * ones(iris.target.size)\n",
    "\n",
    "# evaluate posterior for each point and find maximum\n",
    "for i in range(iris.target.size):\n",
    "    pmax, kmax = -1, None   # initialize to nonsense values\n",
    "    for k in classes:\n",
    "        num, prior, mu, var = param[k]\n",
    "        diff = iris.data[i,:] - mu\n",
    "        d2 = diff*diff / (2*var) \n",
    "        p = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2*pi*var))\n",
    "        if p > pmax:\n",
    "            pmax = p\n",
    "            kmax = k\n",
    "    k_pred[i] = kmax\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=k_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sklearn's version - read up on differences if interested\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (iris.target.size, (iris.target!=y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same results from both methods?\n",
    "np.alltrue(k_pred==y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class probabilities\n",
    "gnb.predict_proba(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still cheating!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "- Features are automatically treated correctly relative to each other\n",
    "\n",
    "> For example, measuring similar things in different units? 1m vs 1mm\n",
    "\n",
    "> The estimated mean and variance puts them on a meaningful scale\n",
    "\n",
    "- Independence is a strong assumption and for no good reason\n",
    "\n",
    "> Actually... it helps with the computational cost!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Gaussian Naive Bayes\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Covariance Matrix\n",
    "\n",
    "- Estimate the full covariance matrix for the classes\n",
    "\n",
    ">$\\displaystyle {\\cal{}L}_{\\boldsymbol{x}}(C_k) =  G(\\boldsymbol{x};\\mu_k, \\Sigma_k)$\n",
    "\n",
    "> Handles correlated features well\n",
    "\n",
    "- Consider binary problem with 2 classes\n",
    "\n",
    "> Taking the negative logarithm of the likelihoods we compare\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1)^T\\,\\Sigma_1^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_1) + \\ln\\,\\lvert\\Sigma_1\\lvert$ vs.\n",
    "\n",
    ">$\\displaystyle (\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2)^T\\,\\Sigma_2^{-1}(\\boldsymbol{x}\\!-\\!\\boldsymbol{\\mu}_2) + \\ln\\,\\lvert\\Sigma_2\\lvert$\n",
    "\n",
    "> If the difference is lower than a threshold, we classify it accordingly\n",
    "\n",
    "- This is called [**Quadratic Discriminant Analysis**](https://scikit-learn.org/stable/modules/lda_qda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same Covariance Matrix\n",
    "\n",
    "- When $\\Sigma_1=\\Sigma_2=\\Sigma$, the quadratic terms cancel from the difference\n",
    " \n",
    ">$\\displaystyle (x\\!-\\!\\mu_1)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_1) $ \n",
    ">$\\displaystyle -\\ (x\\!-\\!\\mu_2)^T\\,\\Sigma^{-1}(x\\!-\\!\\mu_2) $\n",
    "\n",
    "- Hence this is called [**Linear Discriminant Analysis**](https://scikit-learn.org/stable/modules/lda_qda.html)\n",
    "\n",
    "> Fewer parameters to estimate during the learning process\n",
    "\n",
    "> Good, if we don't have enough data, for example...\n",
    "\n",
    "> Think linear vs quadratic fitting and how you decide between those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: QDA and LDA\n",
    "\n",
    "- Use the provided [training](Class-Train.csv) and [query](Class-Query.csv) sets to perform classification\n",
    "\n",
    "> **Training** set consists of 3 columns of ($x_i$, $y_i$, $C_i$)\n",
    "\n",
    "> **Query** set only has 2 columns of ($x_i$, $y_i$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unhomework\n",
    "\n",
    "- Visualize the results in the 2D features space\n",
    "- Make these simple codes run faster by getting rid of the `for` loops, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Summary</font></h1>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "c = np.unique(iris.target)\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure of [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit\n",
    "> Estimate the parameters in each class\n",
    "\n",
    "- Predict\n",
    "> For each unlabeled data, calculate the posterior for each class\n",
    ">\n",
    "> Classify the data with class k having the largest posterior\n",
    "\n",
    "- Assumption\n",
    "> Features are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example for Gaussian Naive Bayes\n",
    "\n",
    "class GNB(dict):\n",
    "    \n",
    "    def fit(self, X, C):\n",
    "        for k in np.unique(C):\n",
    "            # Observation in class k\n",
    "            members = (C == k)\n",
    "            # Number of obvervation in class k\n",
    "            num = members.sum() \n",
    "            # Use frequency as prior\n",
    "            prior = num / float(C.size)\n",
    "            # Choose the observation in class k\n",
    "            XX = X[members,:] \n",
    "            # Calculate mean for class k\n",
    "            mu = XX.mean(axis=0)\n",
    "            # Center\n",
    "            XX -= mu\n",
    "            # Calculate variance for class k\n",
    "            var = (XX*XX).sum(axis=0) / (XX.shape[0]-1)\n",
    "            # Save the result for class k\n",
    "            self[k] = (prior, num, mu, var)\n",
    "    \n",
    "    def predict(self, Y):\n",
    "        pred = -1 * ones(Y.shape[0])\n",
    "        for i in range(pred.size):\n",
    "            # Initialization\n",
    "            pmax, kmax = -1, None   \n",
    "            # Calculate the posterior for each class\n",
    "            for k in self:\n",
    "                prior, num, mu, var = self[k]\n",
    "                diff = Y[i,:] - mu\n",
    "                d2 = diff*diff / (2*var) \n",
    "                posterior = prior * np.exp(-d2.sum()) / np.sqrt(np.prod(2*pi*var))\n",
    "                # Update the threshold and prediction with the largest posterior\n",
    "                if posterior > pmax:\n",
    "                    pmax = posterior\n",
    "                    kmax = k\n",
    "                pred[i] = kmax\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GNB\n",
      "Number of mislabeled points out of a total 150 points : 6\n",
      "Accuracy:  0.96\n"
     ]
    }
   ],
   "source": [
    "clf = GNB()\n",
    "clf.fit(iris.data, iris.target)\n",
    "pred = clf.predict(iris.data)\n",
    "\n",
    "print('Classifier: GNB')\n",
    "print('Number of mislabeled points out of a total %d points : %d' % (iris.target.size, (iris.target!=pred).sum()))\n",
    "print('Accuracy: ', mean(iris.target==pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: GNB\n",
      "Number of mislabeled points out of a total 150 points : 6\n",
      "Accuracy:  0.96\n"
     ]
    }
   ],
   "source": [
    "# Specify the model\n",
    "clf = GaussianNB(priors=None)\n",
    "\n",
    "# Fit\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Predict\n",
    "pred = clf.predict(iris.data)\n",
    "\n",
    "print('Classifier: GNB')\n",
    "print('Number of mislabeled points out of a total %d points : %d' % (iris.target.size, (iris.target!=pred).sum()))\n",
    "print('Accuracy: ', mean(iris.target==pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
